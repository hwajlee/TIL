{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 0. Prepare dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras에서 model 저장할 때 방법 두 가지\n",
    "\n",
    "1. 학습이 완료된 상태에서 model 저장 \n",
    "2. 학습하면서 epoch가 끝날 때마다 파일로 저장 (중간 저장)\n",
    "> 2번 방법의 경우 overfiting이 시작되기 전으로 되돌리거나 혹시나 커널이 꺼졌을 때 꺼지기 이전까지 돌아간 epoch별 model 파일 되살릴 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, preprocessing\n",
    "# 딥러닝 할 때도 sklearn을 통해 train test split, pipeline을 통한 원핫 인코딩 등 적용해야 하므로 해당 라이브러리 사용함 (전처리에 한해서 사용하는 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.read_csv(\"titanic_modified.csv\") # 전처리가 끝난 titanic data load\n",
    "\n",
    "titanic_target = titanic_df[['Survived']].copy()\n",
    "titanic_data = titanic_df.copy()\n",
    "\n",
    "del titanic_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>isAlone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex  Age  Fare  Embarked  Title  FamilySize  isAlone\n",
       "0       3    0  1.0   0.0         0      3         0.4        0\n",
       "1       1    1  2.0   0.0         2      4         0.4        0\n",
       "2       3    1  1.0   0.0         0      2         0.0        1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived\n",
       "0         0\n",
       "1         1\n",
       "2         1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label = model_selection.train_test_split(titanic_data, titanic_target,\n",
    "                                                                                 test_size=0.3,\n",
    "                                                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn에 있는 onehotencoder\n",
    "# enc = preprocessing.OneHotEncoder(categories='auto') \n",
    "# train_label = enc.fit_transform(train_label).toarray()\n",
    "# test_label = enc.fit_transform(test_label).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow.keras에 있는 onehotencoder\n",
    "from tensorflow.keras import utils \n",
    "\n",
    "train_label = utils.to_categorical(train_label)\n",
    "test_label = utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "둘 중에 어떤 것을 써도 관계 x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 1. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, utils\n",
    "from tensorflow.keras import models, layers, activations, initializers, losses, optimizers, metrics\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() \n",
    "\n",
    "# batch normalization 및 flatten 안 해줬으므로 input_dim에 x 데이터 열의 수 적어줘야 함 \n",
    "# 현재 titanic_df에서 x 데이터 열의 수 8개 \n",
    "\n",
    "# 첫 번째 hidden layer \n",
    "model.add(layers.Dense(input_dim=8, units=256, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "# model.add(layers.BatchNormalization()) # 원한다면 Activation 이전에 써줄 것 \n",
    "model.add(layers.Activation('elu')) # elu or relu\n",
    "\n",
    "# 두 번째 hidden layer \n",
    "model.add(layers.Dense(units=512, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu')) \n",
    "\n",
    "# 세 번째 hidden layer \n",
    "model.add(layers.Dense(units=512, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu'))\n",
    "\n",
    "# 네 번째 hidden layer \n",
    "model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu')) \n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "model.add(layers.Dense(units=2, activation='softmax')) # One-hot vector for 0 & 1\n",
    "# 원핫인코딩을 통해 정답 데이터 열의 수가 2개가 되었으므로 units = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(), \n",
    "              loss=losses.categorical_crossentropy, \n",
    "              metrics=[metrics.categorical_accuracy]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose의 값을 0으로 지정하면 silent mode로 fitting이 진행됩니다.\n",
    "\n",
    "history = model.fit(train_data, train_label, batch_size=100, epochs=20, validation_split=0.3, verbose=0) # verbose 수다쟁이 ^-^ 0으로 두면 epoch별 성능지표 수치값 안나옴  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 2. Test the model before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 0.4343 - categorical_accuracy: 0.8172\n",
      "loss (cross-entropy) : 0.43429675698280334\n",
      "test accuracy : 0.8171641826629639\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data, test_label)\n",
    "\n",
    "print('loss (cross-entropy) :', result[0])\n",
    "print('test accuracy :', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 3. Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 528,386\n",
      "Trainable params: 528,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # model의 구조를 보여줌 \n",
    "# 전체 레이어에 거쳐서 추가된 세터들의 총합까지 확인 가능 (total params)\n",
    "# 이때 학습 가능하지 않은 파라미터 세터란? \n",
    "## model.save를 통해 모델을 저장하고 불러올 때 불러오는 대상이 되는 model을 \n",
    "## 우리가 만든 모델이 아닌 학습이 완료된 다른 좋은 모델 가져다 쓸 수 있음 (전이학습)\n",
    "## 그 과정에서 마지막 히든 레이어 한 두개만 추가하고 그 외 나머지 레이어와 파리미터 세터들을 \n",
    "## 꽁꽁 얼려두기 때문에 (아래 코드 참조) 이때 non-trainable params 발생 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 528,386\n",
      "Trainable params: 0\n",
      "Non-trainable params: 528,386\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.trainable = False # 세터들을 학습 가능하지 않은 상태로 만드는 것 \n",
    "# 그리고 그 상태에서 히든 레이어 한 두개 추가로 쌓은 후 추가 학습하면 이게 바로 전이학습\n",
    "# model.fit(~~~)\n",
    "model.summary()\n",
    "# Non-trainable params: 528,386 \n",
    "# 즉, 다시 말해 파라미터 세터들을 동결시킨 것 (비유하자면..)\n",
    "# 동결 후 model.fit을 통해 학습시켜도 학습되지 않음 (세터들이 변하지 않는 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장되는 항목 \n",
    "# - Model의 architecture 모델의 구조 (레이어의 개수, 퍼셉트론의 개수, activation fuction의 종류 등)\n",
    "# - Model의 모든 weights (Parameter Theta) 모델에 포함된 세터들 \n",
    "# - The state of the optimizer (바로 모델 학습의 재개 가능) model.fit할 때 내부적으로 돌아가던 심부름꾼 (Gradient descent)\n",
    "## 이때 optimizer이 저장되어 있으면, model을 불러왔을 때 model.fit을 추가로 하면 \n",
    "## 직전이 학습이 마쳐진 상태에서 이어서 추가학습 가능 \n",
    "\n",
    "model.save('trained_model.h5') # \"Save\" the model\n",
    "# 이때 파일 확장자(h5)는 사실상 아무런 의미 없음 어떠한 역할도 없으므로 지워도 괜찮음\n",
    "\n",
    "# model.save_weights('trained_model.h5') # weights만 따로 저장도 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 4. Load the saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import models\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "model = models.load_model('trained_model.h5') # \"Load\" the \"model\"\n",
    "\n",
    "# model.load_weights('trained_model.h5') # weights만 따로 불러올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6193 - categorical_accuracy: 0.8209\n",
      "loss (cross-entropy) : 0.6193192601203918\n",
      "test accuracy : 0.8208954930305481\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data, test_label)\n",
    "\n",
    "print('loss (cross-entropy) :', result[0])\n",
    "print('test accuracy :', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Appendix) Save the model while training (+ Keras Callbacks API) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Keras Callbacks API (Keras 콜백 함수)를 사용하는 이유 \n",
    "<br>\n",
    "\n",
    "- 모델의 **학습(fit)이 시작된 이후**에는 **학습 완료 전까지 사람이 컨트롤할 수 있는 것이 없음**\n",
    "- 이를 해결하기 위한 도구가 Keras의 콜백 함수 (프로그래밍 전반에서의 콜백 함수 정의 : https://j.mp/3ibaAT4)\n",
    "- ex) 학습 중 Learning rate 값을 변화시키기 / 학습 중 일정 시간 성능 개선이 없을 경우 학습 조기 종료 / 학습 중 모델 중간 저장 등\n",
    "- Keras Callbacks API 공식 문서 : https://keras.io/api/callbacks/\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Keras Callback examples\n",
    "<br>\n",
    "\n",
    "- **ModelCheckpoint** (학습 중간 저장) : https://deep-deep-deep.tistory.com/53 + 아래 코드 참고\n",
    "- **EarlyStopping** (학습 조기 종료) : https://deep-deep-deep.tistory.com/55\n",
    "- **ReduceLROnPlateau** (학습율 자동 조절) : https://deep-deep-deep.tistory.com/56 (Plateau란? @ https://j.mp/3B56FzJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: save_freq (1933838675.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9960\\1933838675.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    tf.keras.callbacks.ModelCheckpoint(\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword argument repeated: save_freq\n"
     ]
    }
   ],
   "source": [
    "# 다양한 ModelCheckpoint 적용 예제 @ https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko#훈련하는_동안_체크포인트_저장하기\n",
    "# 아래 Parameter들의 상세 설명 @ https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "\n",
    "tf.keras.callbacks.ModelCheckpoint( \n",
    "    filepath, # 모델 저장 경로, if '{epoch:02d}-{val_loss:.5f}.h5' -> ex) 01(2자리 epoch 수)-0.39121(해당 epoch val_loss값).h5 \n",
    "    monitor = 'val_loss', # 'loss', 'val_loss', 'accuracy', etc.\n",
    "    verbose = 0, # 0 or 1\n",
    "    save_best_only = False, # True : monitor 중인 지표 기준 가장 좋은 모델 저장 or False : 하단 save_freq 기준 주기적 저장\n",
    "    save_weights_only = False, # True == model.save_weights(filepath) or False == model.save(filepath) \n",
    "    mode = 'auto', # 'auto', 'min', 'max'\n",
    "    save_freq = 'epoch', # 'epoch' or integer(== # of batches) \n",
    "    save_freq = 5 * batch_size # == saves the model's weights every 5 epochs (variable 'batch_size' should be set already)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() \n",
    "model.add(layers.Dense(input_dim=8, units=256, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu')) # elu or relu\n",
    "model.add(layers.Dense(units=512, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu')) \n",
    "model.add(layers.Dense(units=512, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu')) \n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(units=2, activation='softmax')) # One-hot vector for 0 & 1\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(), \n",
    "              loss=losses.categorical_crossentropy, \n",
    "              metrics=[metrics.categorical_accuracy]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2289348346.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9960\\2289348346.py\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    verbose=0)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# 모델 체크포인트 파일(중간 저장 모델)을 저장할 경로 설정 \n",
    "checkpoint_path = 'saved_models/titanic_4-layer_elu.h5' # 필히 saved_models 폴더를 먼저 만들어줘야 합니다\n",
    "\n",
    "# \"ModelCheckpoint\" 콜백함수 객체 생성\n",
    "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                         monitor='val_loss', # default\n",
    "                                                         save_best_only==True, # \"Save\" the \"best\" model \"only\"\n",
    "                                                         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백함수를 호출하며 모델 학습 진행 \n",
    "history = model.fit(train_data, train_label, \n",
    "                    batch_size=100, epochs=100, validation_split=0.3, verbose=0,\n",
    "                    callbacks=[callback_checkpoint]) # 콜백 함수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 학습 완료 시점을 기준으로 한 모델 성능\n",
    "\n",
    "result = model.evaluate(test_data, test_label)\n",
    "print('loss (cross-entropy) :', result[0])\n",
    "print('test accuracy :', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 가장 낮은 Validation loss를 기준으로 한 (저장된) 모델 성능\n",
    "\n",
    "model = models.load_model('saved_models/titanic_4-layer_elu.h5') # \"Load\" the \"model\"\n",
    "\n",
    "result = model.evaluate(test_data, test_label)\n",
    "print('loss (cross-entropy) :', result[0])\n",
    "print('test accuracy :', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
